\chapter{Neural Networks}

{\sf The inspiration to Neural Networks mashines comes from biological neural networks. In 1950's we have already known it works and the main thing that allows a neuron to filter all the useless information of the useless signal that happend in the brain all the time is all-or-none law: if we take incoming membrane potential [pic. 6.1] and outgoing signal at some point if we apply electrical signal to the neuron before some threshold it just deminishes and after some threshold is reached we actually have an inneration of the neuron -- increasing the membrane potention. And after that it does't matter how much stimuls it gets, for example, if the signal is twice the threshold the output signal will be the same.}

\section{Perceptron}

So the perceptron works the same. However, it can be represented as the small neural network where input neurons are the signals, they multiplied by the $w$ coefficients, add it up and if they are over some threshold then we get the signal 1 otherwise we get -1. The problem with that is we can only have one layer. If you combine several layers as how it happens in the brain you have no good way of training the multi-layer perceptron with the simple threshold function. So the way around it is replace the threshold function for something more complex.

\section{Single Layer Networks}

The first aproach is a single layer neural network with logistic function as a threshold. And the single layer neural network with logistic function is called logistic regression (logistic regression is the classification algorithm, it is not the regression algorithm). for the binary classification ($y_1=1$ $y_2=-1$) the math comes out very nicely such that the probability of the signal is the one minus the probability of the reverse signal and if we take that function it satisficed both parameters. So this is how logistic function looks [pic.]\\
{\it <Some formulas>}\\
So the probability $P(y|x)$ we can involded just by multiplying the answer of the perceptron $w^Tx$ by $y$:
$P(y|x)=\sigma(yw^Tx)$
So if our output is the probability of classes how would we calculate the loss function, how we would caluclate the error that we can try to minimize. Well we just use likelihood:
$$\prod\limits_{i=1}^{N}P(y_i|x_i)=\prod\limits_{i=1}^{N}\sigma(y_iw^Tx_i)$$
Then we go from multiplication to sum of the logarithms and get loss function [которую мы уже будем максимизировать] for the logistic regression:
$$L(w)=-\frac{1}{N}\ln\left(\prod\limits_{i=1}^N\sigma(y_iw^Tx_i)\right)=\frac{1}{N}\sum\limits_{i=1}^N\ln(1+e^{-y_iw^Tx_i})$$

Now the problem is training it. Even for one layer logistic regression we need to utilise the gradient descent:
$$w(t+1)=w(t)-\eta\frac{\partial C(w)}{\partial w}$$
The $C(w)$ is the cost function (instead it we can use loss function; we typically talk about the loss function $L(w)$ (or $J(w)$) and the cost function $C(w)$). And we have the learning rate $\eta$.

In case of a large data it becomes hard to calculate the average value: if everything will be perfect we will calculate the gradient of the sum of all the points, so we sum all the gradients and take the average gradient of that points. Hovewer we can go around it by using  a stochastic gradient descent on a sample of the data (batch). It calculates the loss function of all precidious for whole of the data but rather just calculates the batch and then upply the gradient descent step for the batch. <...>
Once we go through all batches so how the algorithm of training the neural network is we first saparate your data in random batches and then we go through all bathes and complete the gradient descent step for every batch you complete the epoch. The number os steps in one epoch is number of all points divided by the size of the betch ($N / N_{batch}$).
So this is the difference beteen the gradient descent and stochastic gradient descent:
$$\text{Gradient Descent: }w\leftarrow w-\eta\left(-\frac{1}{N}\sum\limits_{i=1}^N\frac{y_ix_i}{1+e^{y_iw^Tx_i}}\right)$$
$$\text{Stochastic Gradient Descent: }w\leftarrow w-\eta\left(-\frac{1}{N_{batch}}\sum\limits_{x_i\in batch}\frac{y_ix_i}{1+e^{y_iw^Tx_i}}\right)$$
Also we have a very stochastic gradient descent (when every bench contains only one point):
$$w\leftarrow w-\eta\frac{\partial C(w^Tx_i,y_i)}{\partial w}$$
{\it <The reason why very stochastic gradients descent is not he best way>}

\section{Several Layers Networks}

So we have the algorithm for training in one layer neural network. The way to trainig the several layers is back-propagation. 

\subsubsection*{Back-propagation}

At first we need to define some coefficients and numbers:
\begin{enumerate}[label=$\bullet$]
  \item $w_{jk}^l$ is the weight between neuron $j$ in layer $l-1$ and neuron $k$ in layer $l$.
  \item $x_j^l$ is the outcoming signal after the activation function, for the logistic regression it is $x_j^l=\sigma\left(\sum w_{ij}^lx_i^{l-1}\right)$, in the first layer $x_j^1=x_j$ ($x_j$ is just a feature). You can look at all $x_i^l$ in other layers at the features as well (sometimes they called feature layers).
  \item $s_j^l$ is the incoming signal to neuron $j$ in layer $l$, $s_j^l=\sum w_{ij}^lx_i^{l-1}$
\end{enumerate}
So the values what we try to optimise is w's. We want to calculate the gradient for loss functions over the w's. The goldest to calculate for every particular weight to calculate the partial derilivative of our loss function:
$$\nabla C_{w_{ij}^l}(w)=\frac{\partial C(w)}{\partial w_{ij}^l}=\frac{}{\partial s_j^l}\times\frac{\partial s_j^l}{\partial w_{ij}^l}$$
Now let's see, that
$$\frac{\partial s_j^l}{\partial w_{ij}^l}=x_i^{l-1}$$
because $s_j^l=\sum w_{ij}^lx_i^{l-1}$. Then we calculate $\delta_j^l$ what is defined as
$$\delta_j^l=\frac{\partial C(w)}{\partial s_j^l}$$
If the loss function for the last layer $L$ is diferantionable then we just calculate $\delta_i^L$:
$$\delta_i^L=\frac{\partial C(w)}{\partial s_i^L},\ C(w)=f(x^L),\ x_i^L=\sigma(s_i^L)$$
For the previous layers we can again apply the chain rule and calculate the $\delta_i^{l-1}$ using $\delta_i^l$:
$$\delta_i^{l-1}=\frac{\partial C(w)}{\partial s_i^{l-1}}=\sum\limits_{j}\frac{\partial C(w)}{\partial s_j^l}\times\frac{\partial s_j^l}{\partial x_i^{l-1}}\times\frac{\partial x_i^{l-1}}{\partial s_i^{l-1}}=\sum\limits_{j}\delta_j^l\times w_{ij}^l\times\sigma'(s_i^{l-1})$$
So back-propagation algorithm looks like this:
\begin{enumerate}[label=\arabic*.]
  \item Initialize weights randomly (with small random numbers in $[-0.1,0.1]$).
  \item Compute the forward path: calculate all $x$ and $s$.
  \item Go backward and calculate all $\delta$.
  \item Shift all weights: $w_{ij}^l\leftarrow w_{ij}^l-\eta x_i^{l-1}\delta_j^l$ ($\eta$ is some learning rate).
  \item Go to step 2.
\end{enumerate}
The important thing is you can calclulate all $\delta$ with thensors: you don't have to iterate over each coefficient but rather multiply thensors. Which is good because we can push it into the GPU's what are really good in multiplying thensors.

\subsubsection*{Activation functions}

\begin{enumerate}[label=$\bullet$]
  \item Sigmoid: $\sigma(s)=\frac{1}{1+e^{-s}}$
  \item Tanh: $\sigma(s)=\frac{e^s-e^{-s}}{e^s+e^{-s}}$
  \item Rectified Linear Unit (ReLU): $\sigma(s)=\max(0,s)$
\end{enumerate}
The sigmoid finction goes from 0 to 1. If you want something what can be negative you can use hyperbolic tangens what goes from -1 to 1. And if you what something what calculates very fast and not gets your gradient deminished when you reach big numbers you can use ReLU.

\subsubsection*{Softmax and cross-entropy}

In case of binary classification you use logistic function and you have one output neuron what gets the probability of one class. In multiclass classification we utilize the function what is called the Softmax:
$$x_j^L=\sigma^L(s_j^L)=\frac{e^{s_j^L}}{\sum e^{s_i^L}}$$
It is called Softmax because all last layer neurons output numbers around 0 (exept one that outputs number around 1).\\
The loss function is:
$$C(w)=\sum o_i\log(x_i^L),\ o_i=\begin{cases}
1,&\text{if}y=i\\
0,&\text{if}y\ne i
\end{cases}$$
{<\it Some talking that I can't understand>}\\
The networks what are orginised like that is called fully conected network: each neuron of the one layer connects with each neuron of the next layer and so on.

\section{Regularization}

Networks, espesualy fully conected, are very prompt to overfitting. For example let's say we have a very big network and train it to recognize images. Instead of learning features of the images for example recognizing a cat from a car by ears and so on, big neural networks can just memorize the images: it can looks on the value of concrete pixel. This is an overfitting -- memorising the dataset, trainig to noise and so on. How we can to regularize it? 

\subsubsection*{L2 regularization}

When we regularize we trying to lower the VC-dimention, lower the number of possible hypotheses. We do that by adding some constraints to the loss function. Here we add the L2 constraint:
$$C_{new}(w)=C(w)+\frac{\lambda}{2N}\|w\|_2^2$$
This way is called the L2 regularization or weight decay. If you look at the derilivative of the funсtion $C_{new}$ you will have derilivative of the loss function $C(w)$ and the negative weight. <...>

\subsubsection*{Early stopping}

The another way in reguralization is catching the moment of overfitting. So this is a typical graph what shows gain between errors on the validation dataset and training dataset. The x axis is epochs.\\
The error on the training dataset is always decrease, but error in the validation dataset in some point starts to increase. And at that point we want to stop (remember the weights of the network in that point).

\subsubsection*{Dropout}

Dropout is direct way to preventing networks from memorizing the dataset. We just turn of some (50\%, for example) random neurons for every layers in every patch. The turned of neuron outputs 0. In that way the network will try to learn general features rather than specific pathways, because this pathway can be turned off.\\
When we apply this network at some task we need to turn on all the neurons.

\subsubsection*{Data augmentation}

The overfitting doesn't happen if you have a lot of data. How to create a lot of data we will talk at the next lecture.

\section{Deep Learning Libraries}

If you don't like Python learn to like Python because most of deep learning done in Python. However there is still some inthusiasts what work on the deep learning for Java \href{http://deeplearning4j.org}{library}.\\
Google represents \href{https://www.tensorflow.org/}{TensorFlow} library and the \href{keras.io}{Keras} library is built on top of that. So if you just want to create networks simply you should use Keras. And if you want to create complex networks you should use TensorFlow.\\
The \href{torch.ch}{Torch} library easier than TensorFlow but more complex than Keras.